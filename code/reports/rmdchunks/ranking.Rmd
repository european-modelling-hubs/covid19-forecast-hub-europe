<!--- Table comparing the performance of each model. The table is followed by
an explanation text detailing the meaning of each column/index. --->

## Forecast scores {.tabset .tabset-fade}

Scores separated by target and forecast horizon. Only models with submissions in each of the last `r restrict_weeks` weeks are shown.

```{r ranking, results='asis'}
out <- NULL
for (variable in names(target_variables)) {
  out <- c(out, paste("\n\n###", variable, " {.tabset .tabset-fade}\n\n"))
  for (this_horizon in horizons) {
    horizon_string <-
      paste0(this_horizon, " week",
             if_else(this_horizon > 1, "s", ""), " ahead horizon")
    out <- c(out, paste("\n\n####", horizon_string, " {.tabset .tabset-fade}\n\n"))
    weeks_included <- unique(table$weeks_included)
    ## sort weeks included numerically, ensuring non-numeric ("All") comes first
    weeks_included <-
      weeks_included[rev(order(suppressWarnings(as.integer(weeks_included))))]
    for (this_weeks in weeks_included) {
      weeks_string <-
        if_else(grepl("^[0-9]", this_weeks),
		paste(this_weeks, "weeks history"),
		paste(this_weeks, "history"))
        out <- c(out, paste("\n\n#####", weeks_string, "\n\n"))
        out <- c(out, knit_child("ranking-table.Rmd", quiet = TRUE))
    }
  }
}
res <- knit(text = out, quiet = TRUE)
cat(res, sep = '\n\n')
```

## {.unlisted .unnumbered}

---

## Evaluation metrics

```{r description, results='asis'}
if ("n" %in% colnames(df)) {
  cat("- The column `n` gives the number of forecasts included in the evaluation. This number may vary across models as some models joined later than others, or models may not have submitted forecasts in certain weeks.\n")
}
if ("n_loc" %in% colnames(df)) {
  cat("- The column `n_loc` gives the number of different locations being included in the shown evaluation. For invdividual country reports this is 1, as we do not currently consider subnational forecasts.\n")
}
if ("rel_wis" %in% colnames(df)) {
  cat("- The [weighted interval score](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618) (WIS) is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule#Propriety) (i.e., it cannot be \"cheated\") that is suited to scoring forecasts in an interval format. It generalizes the absolute error (i.e. lower values are better) and has three components: dispersion, underprediction and overprediction. Dispersion is a weighted average of the widths of the submitted prediction intervals. Over- and underprediction (overpred/underpred) penalties are added whenever an observation falls outside of a reported central prediction interval, with the strength of the penalty depending on the nominal level of the interval and how far outside of the interval the observation fell. Note that the average WIS can refer to different sets of targets for different models and therefore _cannot always be compared across models_. Such comparisons should be done based on the relative skill.  \n")
  cat("The Relative WIS (column `rel_wis`) is a relative measure of forecast performance which takes into account that different teams may not cover the exact same set of forecast targets (i.e., weeks and locations). Loosely speaking, a relative WIS of X means that averaged over the targets a given team addressed, its WIS was X times higher/lower than the the performance of the baseline model described in [Cramer et al. (2021)](https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1). Smaller values are thus better and a value below one means that the model has above average performance. The relative WIS is computed using a 'pairwise comparison tournament' where for each pair of models a mean score ratio is computed based on the set of shared targets. The relative WIS is the geometric mean of these ratios. Details on the computation can be found in [Cramer et al. (2021)](https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1).  \n")
  cat("This metric is calculated for all models that provide a [full set](https://github.com/epiforecasts/covid19-forecast-hub-europe/wiki/Forecast-format#quantile) of quantiles.\n")
}
if ("rel_ae" %in% colnames(df)) {
  cat("- The Relative Absolute Error (column `rel_ae`) is the relative absolute error of the predictive point forecasts, that is the predicted value that individual forecasts deem the most likely. The relative AE is computed using a 'pairwise comparison tournament' where for each pair of models a mean score ratio is computed based on the set of shared targets. The relative AE is the geometric mean of these ratios. Details on the computation can be found in [Cramer et al. (2021)](https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1).\n")
}
if (length(grep("^cov_", colnames(df))) > 0) {
  cat("- Coverage (`50% Cov.` / `95% Cov.`) is the proportion of observations that fell within a given prediction interval. Ideally, a forecast model would achieve 50% coverage of 0.50 (i.e., 50% of observations fall within the 50% prediction interval) and 95% coverage of 0.95 (i.e., 95% of observations fall within the 95% prediction interval). Values of coverage greater than these nominal values indicate that the forecasts are _underconfident_, i.e. prediction intervals tend to be too wide, whereas values of coverage smaller than these nominal values indicate that the forecasts are _overconfident_, i.e. prediction intervals tend to be too narrow.  \n")
  cat("This metric is calculated for all models that provide the relevant quantiles.\n")
}
if ("bias" %in% colnames(df)) {
  cat("- Bias (`bias`) is a measure between -1 and 1 that expresses the tendency to underpredict (-1) or overpredict (1), see the description in [Funk et al. (2019)](https://doi.org/10.1371/journal.pcbi.1006785).  \n")
  cat("This metric is calculated for all models that provide quantiles.\n")
}
```

## {.unlisted .unnumbered}
