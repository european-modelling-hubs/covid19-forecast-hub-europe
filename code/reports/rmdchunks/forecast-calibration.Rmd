<!--- 
- Table with coverage by horizon for this week
- Plot of coverage by target and horizon over time
- PIT histograms
--->
# Forecast calibration

The table and plot below show this week's _coverage_ of the ensemble model at the 50% and 95% level, across the 32 countries. This shows the proportion of observations that fall within a given prediction interval. Ideally, a forecast model would achieve 50% coverage of 0.50 (i.e., 50% of observations fall within the 50% prediction interval) and 95% coverage of 0.95 (i.e., 95% of observations fall within the 95% prediction interval). Values of coverage greater than these nominal values indicate that the forecasts are _underconfident_, i.e. prediction intervals tend to be too wide, whereas values of coverage smaller than these nominal values indicate that the ensemble forecasts are _overconfident_, i.e. prediction intervals tend to be too narrow.

## Overall coverage

```{r coverage, echo = FALSE, include = include_calibration, results = 'asis', fig.width = 5, fig.height = 5}
ranges <- c(50, 90)

scores <- scoringutils::eval_forecasts(
  data,
  summarise_by = c("model", "range", "quantile",
                  "target_variable", "horizon"),
  pit_plots = TRUE
  ) %>%
  filter(!is.na(range))

if (nrow(scores) > 0) {
  coverage <- scores %>%
    dplyr::filter(range %in% ranges) %>%
    select(range, target_variable, horizon, coverage) %>%
    distinct() %>%
    mutate(horizon = as.integer(horizon),
           range = paste0(range, "% interval"),
           target_variable = recode_factor(target_variable,
                                           `inc case` = "Cases",
                                           `inc death` = "Deaths"),
           coverage = round(coverage, 2))

  hlines <- tibble(range = paste0(ranges, "% interval"),
                   nominal = ranges / 100)

  p <- ggplot(coverage, aes(y = coverage)) +
    geom_line(aes(x = horizon)) +
    geom_point(aes(x = horizon)) +
    ylim(c(0, 1)) +
    geom_hline(data = hlines, aes(yintercept = nominal), linetype = "dashed") +
    facet_grid(range ~ target_variable) +
    theme_minimal() +
    ylab("Proportion of data within forecast interval") +
    xlab("Forecast horizon (weeks )")
  print(p)
} else {
  cat("No coverage figures shown as now 50%/90% predictive intervals are available.\n")
}
```

## PIT histograms

```{r pit_width, echo = FALSE}
width <- 0.05
```

The figures below are _PIT histograms_ for the all past forecasts. These show the proportion of true values within each predictive quantile (width: `r width`). If the forecasts were perfectly calibrated, observations would fall evenly across these equally-spaced quantiles, i.e. the histograms would be flat.

```{r pit, echo = FALSE, results = 'asis', include = include_calibration}
if (nrow(scores) > 0) {
  ## check if all quantiles are present
  quantiles <- round(seq(width, 1 - width, by = width), 3)

  core_quantiles <- scores %>%
    mutate(quantile = round(quantile, 3)) %>%
    summarise(quantile = unique(quantile))

  if (length(setdiff(quantiles, core_quantiles$quantile)) == 0) {
    pit <- scores %>%
      filter(round(quantile, 3) %in% round(quantiles, 3)) %>%
      mutate(horizon = paste0(horizon, " week", if_else(horizon == 1, "", "s"))) %>%
      arrange(target_variable, horizon, quantile) %>%
      group_by(target_variable, horizon) %>%
      summarise(quantile = c(quantile, 1),
                pit_bin = diff(c(0, quantile_coverage, 1)))

    p <- ggplot(pit, aes(x = quantile - width / 2, y = pit_bin)) +
      geom_col() +
      theme_light() +
      facet_grid(horizon ~ target_variable) +
      xlab("Quantile") + ylab("Proportion") +
      geom_hline(yintercept = width, linetype = "dashed")

    print(p)
  } else {
    cat("No PIT histogram shown because not all required predictive quantiles are available.\n")
  }
} else {
    cat("No PIT histogram shown because no predictive quantiles are available.\n")
}
```
